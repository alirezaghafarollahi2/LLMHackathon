{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12fd009-464b-4c5a-b71b-daba40e75206",
   "metadata": {},
   "source": [
    "# LLM agent Hackathon "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074a789-f0f8-4473-be2b-b1c072fb6db8",
   "metadata": {},
   "source": [
    "## Alireza Ghafarollahi, MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645493f-a6ab-4184-afe9-a17db1757808",
   "metadata": {},
   "source": [
    "# Create your first agent\n",
    "- ## LLM: GPT-4o\n",
    "- ## Task: write a Python code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1232795b-dfe0-4953-b836-041f5207096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "OPENAI_API_KEY = config[\"api_key\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a6051ef-4ceb-43df-88ac-6403135ca22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os.path as osp\n",
    "import subprocess\n",
    "\n",
    "client = OpenAI(organization ='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "191f2ff7-dc95-44e0-9dab-326be5be7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_between_markers(llm_output):\n",
    "    # Regular expression pattern to find JSON content between ```json and ```\n",
    "    json_pattern = r\"```json(.*?)```\"\n",
    "    matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
    "\n",
    "    if not matches:\n",
    "        # Fallback: Try to find any JSON-like content in the output\n",
    "        json_pattern = r\"\\{.*?\\}\"\n",
    "        matches = re.findall(json_pattern, llm_output, re.DOTALL)\n",
    "\n",
    "    for json_string in matches:\n",
    "        json_string = json_string.strip()\n",
    "        try:\n",
    "            parsed_json = json.loads(json_string)\n",
    "            return parsed_json\n",
    "        except json.JSONDecodeError:\n",
    "            # Attempt to fix common JSON issues\n",
    "            try:\n",
    "                # Remove invalid control characters\n",
    "                json_string_clean = re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", json_string)\n",
    "                parsed_json = json.loads(json_string_clean)\n",
    "                return parsed_json\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Try next match\n",
    "\n",
    "    return None  # No valid JSON found\n",
    "    \n",
    "def get_response_from_llm(\n",
    "        system_message,\n",
    "        prompt,\n",
    "        model,\n",
    "        reasoning_effort=\"medium\",\n",
    "        print_debug=False,\n",
    "        msg_history=None,\n",
    "        temperature=0.75,\n",
    "        client=client):\n",
    "\n",
    "    if msg_history is None:\n",
    "        msg_history = []\n",
    "\n",
    "    new_msg_history = msg_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    if model in [\"gpt-4o\", \"gpt-4-turbo\"]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": system_message},\n",
    "                *new_msg_history,\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_completion_tokens=15000\n",
    "        )\n",
    "        print(token_usage(response))\n",
    "\n",
    "    elif model in [\"gpt-4.1\"]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": system_message},\n",
    "                *new_msg_history,\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_completion_tokens=20000\n",
    "        )\n",
    "        print(token_usage(response))\n",
    "        \n",
    "    elif model in [\"o1\", \"o1-mini\", \"o3\", \"o3-mini\"]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": system_message},\n",
    "                *new_msg_history,\n",
    "            ],\n",
    "        )\n",
    "        print(token_usage(response))\n",
    "        \n",
    "    content = response.choices[0].message.content\n",
    "    new_msg_history = new_msg_history + [{\"role\": \"assistant\", \"content\": content}]\n",
    "\n",
    "    if print_debug:\n",
    "        print()\n",
    "        print(\"*\" * 20 + \" LLM START \" + \"*\" * 20)\n",
    "        for j, msg in enumerate(new_msg_history):\n",
    "            print(f'{j}, {msg[\"role\"]}: {msg[\"content\"]}')\n",
    "            print(\"*\" * 51)\n",
    "        #print(content)\n",
    "        print(\"*\" * 21 + \" LLM END \" + \"*\" * 21)\n",
    "        print()\n",
    "\n",
    "    return content, new_msg_history\n",
    "\n",
    "def token_usage(response):\n",
    "    dic = {\"prompt tokens\": response.usage.prompt_tokens,\n",
    "          \"completion tokens\" : response.usage.completion_tokens,\n",
    "          \"total tokens\": response.usage.total_tokens,\n",
    "           \"reasoning tokens\": response.usage.completion_tokens_details.reasoning_tokens\n",
    "          }\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75eb2ea6-4962-4f05-b838-2f2ad0269ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt tokens': 162, 'completion tokens': 439, 'total tokens': 601, 'reasoning tokens': 0}\n",
      "\n",
      "******************** LLM START ********************\n",
      "0, user:  You carefully write a Python code for the given task.\n",
      "\n",
      "INSTRUCTION:\n",
      "- Always save the plots, rather than showing them.\n",
      "\n",
      "Respond in the following format\n",
      "\n",
      "<code_START>\n",
      "<CODE>\n",
      "<code_FINISH>\n",
      "\n",
      "Replace <CODE> with your complete and well-organized Python script. \n",
      "\n",
      "Additional note:\n",
      "Write just the raw Python logic, no Markdown formatting, and no main() function wrapper, no ```python in the beginning and ``` in the end.\n",
      "\n",
      "TASK:\n",
      "Derive the mathematical expression for the roots of a third degree polynomial. \n",
      "Then give a sample function and compute its roots using the expressions. Finally, plot the function, showing the roots and annotaing the mathematical expression.\n",
      "\n",
      "***************************************************\n",
      "1, assistant: <code_START>\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sympy import symbols, solve, Eq, latex\n",
      "\n",
      "# Define the symbols\n",
      "x = symbols('x')\n",
      "\n",
      "# Define the coefficients of the polynomial\n",
      "a, b, c, d = 1, -6, 11, -6  # Example: x^3 - 6x^2 + 11x - 6\n",
      "\n",
      "# Define the polynomial\n",
      "polynomial = a*x**3 + b*x**2 + c*x + d\n",
      "\n",
      "# Solve for the roots\n",
      "roots = solve(Eq(polynomial, 0), x)\n",
      "\n",
      "# Convert roots to numerical values\n",
      "roots_numeric = [float(root.evalf()) for root in roots]\n",
      "\n",
      "# Create a function to evaluate the polynomial\n",
      "def f(x_val):\n",
      "    return a*x_val**3 + b*x_val**2 + c*x_val + d\n",
      "\n",
      "# Generate x values for plotting\n",
      "x_vals = np.linspace(min(roots_numeric) - 1, max(roots_numeric) + 1, 400)\n",
      "y_vals = f(x_vals)\n",
      "\n",
      "# Plot the function\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.plot(x_vals, y_vals, label='Polynomial Function')\n",
      "plt.axhline(0, color='black', linewidth=0.8)\n",
      "\n",
      "# Plot the roots\n",
      "for root in roots_numeric:\n",
      "    plt.plot(root, 0, 'ro')  # Red dot for root\n",
      "    plt.annotate(f'Root: {root:.2f}', (root, 0), textcoords=\"offset points\", xytext=(-15,10), ha='center')\n",
      "\n",
      "# Annotate the mathematical expression\n",
      "expression = f'${latex(polynomial)} = 0$'\n",
      "plt.annotate(expression, (0.5, 0.9), xycoords='axes fraction', fontsize=12, ha='center')\n",
      "\n",
      "# Add labels and legend\n",
      "plt.title('Plot of the Polynomial and its Roots')\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('f(x)')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "\n",
      "# Save the plot\n",
      "plt.savefig('polynomial_roots_plot.png')\n",
      "<code_FINISH>\n",
      "***************************************************\n",
      "********************* LLM END *********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message = '''You are a senior coder with expertise in writing Python codes. \n",
    "'''\n",
    "\n",
    "prompt = ''' You carefully write a Python code for the given task.\n",
    "\n",
    "INSTRUCTION:\n",
    "- Always save the plots, rather than showing them.\n",
    "\n",
    "Respond in the following format\n",
    "\n",
    "<code_START>\n",
    "<CODE>\n",
    "<code_FINISH>\n",
    "\n",
    "Replace <CODE> with your complete and well-organized Python script. \n",
    "\n",
    "Additional note:\n",
    "Write just the raw Python logic, no Markdown formatting, and no main() function wrapper, no ```python in the beginning and ``` in the end.\n",
    "\n",
    "TASK:\n",
    "{task}\n",
    "'''\n",
    "\n",
    "task = '''Derive the mathematical expression for the roots of a third degree polynomial. \n",
    "Then give a sample function and compute its roots using the expressions. Finally, plot the function, showing the roots and annotaing the mathematical expression.''' \n",
    "\n",
    "code_hosory = []\n",
    "\n",
    "code, code_history = get_response_from_llm(system_message=system_message,\n",
    "                                          model='gpt-4o',\n",
    "                                          prompt=prompt.format(task=task),\n",
    "                                          temperature=0,\n",
    "                                          print_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e513e751-5561-459f-8b6d-2105edcdfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r'<code_START>(.*?)<code_FINISH>', code, re.DOTALL)\n",
    "if match:\n",
    "    code_block = match.group(1).strip()\n",
    "else:\n",
    "    print(\"Code block not found.\")\n",
    "    \n",
    "with open(f'python_code.py', 'w') as f:\n",
    "    f.writelines(code_block)\n",
    "        \n",
    "command = ['python', f\"python_code.py\"]            \n",
    "    \n",
    "result = subprocess.run(command, stderr=subprocess.PIPE, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4828c439-093c-43be-a6e6-286727acf0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['python', 'python_code.py'], returncode=0, stderr=\"findfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXSizeOneSym'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXSizeTwoSym'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXSizeThreeSym'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXSizeFourSym'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['STIXSizeFiveSym'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmsy10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmr10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmtt10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmmi10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmb10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmss10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['cmex10'] not found. Falling back to DejaVu Sans.\\nfindfont: Font family ['DejaVu Sans Display'] not found. Falling back to DejaVu Sans.\\n\")\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b4313-626c-42ca-b9f6-f5595f1cd127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
